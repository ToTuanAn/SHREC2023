{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORKDIR = \"/home/totuanan/Workplace/SHREC2023/SHREC2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/totuanan/Workplace/SHREC2023/SHREC2023\n",
      "Processing /home/totuanan/Workplace/SHREC2023/SHREC2023\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hBuilding wheels for collected packages: shrec2023\r\n",
      "  Building wheel for shrec2023 (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for shrec2023: filename=shrec2023-0.0.1-py3-none-any.whl size=21936 sha256=5bf7fcd7ea4adc4282bd5241c940e8d72ccb988a857bc7ebd04798de30f6a4dd\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6705irx3/wheels/f5/1f/57/47ee352a6c80510cf3ee381584bdb9085248a82c4f0a7b0649\r\n",
      "Successfully built shrec2023\r\n",
      "Installing collected packages: shrec2023\r\n",
      "  Attempting uninstall: shrec2023\r\n",
      "    Found existing installation: shrec2023 0.0.1\r\n",
      "    Can't uninstall 'shrec2023'. No files were found to uninstall.\r\n",
      "Successfully installed shrec2023-0.0.1\r\n"
     ]
    }
   ],
   "source": [
    "%cd $WORKDIR\n",
    "!pip install ."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/totuanan/Workplace/SHREC2023/SHREC2023\n",
      "/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\r\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\r\n",
      "Overriding configurating\r\n",
      "Global seed set to 1337\r\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\r\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mtotuanan06\u001B[0m (\u001B[33mhcmus-polars\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.13.10\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in \u001B[35m\u001B[1m./runs/wandb/run-20230307_221019-ni80uzbh\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run \u001B[1m`wandb offline`\u001B[0m to turn off syncing.\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33mbce-pc-txt-20230307-221017\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: ‚≠êÔ∏è View project at \u001B[34m\u001B[4mhttps://wandb.ai/hcmus-polars/hcmus-shrec23-textANIMAR\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: üöÄ View run at \u001B[34m\u001B[4mhttps://wandb.ai/hcmus-polars/hcmus-shrec23-textANIMAR/runs/ni80uzbh\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\r\n",
      "/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:467: LightningDeprecationWarning: Setting `Trainer(gpus=-1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=-1)` instead.\r\n",
      "  rank_zero_deprecation(\r\n",
      "Using 16bit None Automatic Mixed Precision (AMP)\r\n",
      "GPU available: True (cuda), used: True\r\n",
      "TPU available: False, using: 0 TPU cores\r\n",
      "IPU available: False, using: 0 IPUs\r\n",
      "HPU available: False, using: 0 HPUs\r\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\r\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\r\n",
      "\r\n",
      "  | Name           | Type          | Params\r\n",
      "-------------------------------------------------\r\n",
      "0 | pointnet       | PointNet      | 3.5 M \r\n",
      "1 | lang_extractor | LangExtractor | 109 M \r\n",
      "2 | lang_linear    | Sequential    | 393 K \r\n",
      "3 | pc_linear      | Linear        | 32.9 K\r\n",
      "4 | fc             | Sequential    | 257   \r\n",
      "-------------------------------------------------\r\n",
      "4.0 M     Trainable params\r\n",
      "109 M     Non-trainable params\r\n",
      "113 M     Total params\r\n",
      "226.873   Total estimated model params size (MB)\r\n",
      "Sanity Checking: 0it [00:00, ?it/s]/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\r\n",
      "  rank_zero_warn(\r\n",
      "Sanity Checking DataLoader 0:   0%|                      | 0/24 [00:00<?, ?it/s]/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\r\n",
      "  warning_cache.warn(\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/totuanan/Workplace/SHREC2023/SHREC2023/src/train.py\", line 56, in <module>\r\n",
      "    train(cfg)\r\n",
      "  File \"/home/totuanan/Workplace/SHREC2023/SHREC2023/src/train.py\", line 50, in train\r\n",
      "    trainer.fit(model, ckpt_path=config[\"global\"][\"resume\"])\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 608, in fit\r\n",
      "    call._call_and_handle_interrupt(\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py\", line 38, in _call_and_handle_interrupt\r\n",
      "    return trainer_fn(*args, **kwargs)\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\r\n",
      "    self._run(model, ckpt_path=self.ckpt_path)\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1103, in _run\r\n",
      "    results = self._run_stage()\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1182, in _run_stage\r\n",
      "    self._run_train()\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1195, in _run_train\r\n",
      "    self._run_sanity_check()\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1267, in _run_sanity_check\r\n",
      "    val_loop.run()\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\r\n",
      "    self.advance(*args, **kwargs)\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 152, in advance\r\n",
      "    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\r\n",
      "    self.advance(*args, **kwargs)\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 137, in advance\r\n",
      "    output = self._evaluation_step(**kwargs)\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 234, in _evaluation_step\r\n",
      "    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1485, in _call_strategy_hook\r\n",
      "    output = fn(*args, **kwargs)\r\n",
      "  File \"/home/totuanan/anaconda3/envs/cvpr_ntl/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py\", line 390, in validation_step\r\n",
      "    return self.model.validation_step(*args, **kwargs)\r\n",
      "  File \"/home/totuanan/Workplace/SHREC2023/SHREC2023/src/model/abstract.py\", line 92, in validation_step\r\n",
      "    g_emb=output[\"true_pc_embedding_feats\"].float(),\r\n",
      "KeyError: 'true_pc_embedding_feats'\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Waiting for W&B process to finish... \u001B[31m(failed 1).\u001B[0m Press Control-C to abort syncing.\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: üöÄ View run \u001B[33mbce-pc-txt-20230307-221017\u001B[0m at: \u001B[34m\u001B[4mhttps://wandb.ai/hcmus-polars/hcmus-shrec23-textANIMAR/runs/ni80uzbh\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Find logs at: \u001B[35m\u001B[1m./runs/wandb/run-20230307_221019-ni80uzbh/logs\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "%cd $WORKDIR\n",
    "!python src/train.py -c configs/bce_pc_txt.yml"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
